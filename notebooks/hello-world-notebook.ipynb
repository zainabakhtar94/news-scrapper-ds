{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "# Set file\n",
    "logFile = \"/home/jovyan/work/data/airflow.cfg\"\n",
    "\n",
    "# Read file\n",
    "logData = sc.textFile(logFile).cache()\n",
    "\n",
    "# Get lines with A\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "\n",
    "# Get lines with B \n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "\n",
    "# Print result\n",
    "print(\"Lines with a: {}, lines with b: {}\".format(numAs, numBs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='classifierdl_use_emotion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"\"\"600,000 flood victims have been shifted to camps: Murad\"\"\",\n",
    "            \"\"\"COAS hails US support for flood victims\"\"\",\n",
    "            \"\"\"My soul has just been pierced by the most evil look from @rickosborneorg. A mini panic attack &amp; chill in bones followed soon after.\"\"\",\n",
    "            \"\"\"Breach in Mirpur-Kotli highway on periphery of Mangla lake: officials\"\"\",\n",
    "            \"\"\"Two dead in SNGPL pipeline blast\"\"\",\n",
    "            \"\"\"Tire erupts at MQM’s former headquarters Nine Zero\"\"\",\n",
    "            \"\"\"A woman was gang raped by three persons including two policemen in Chuhng area\"\"\",\n",
    "            \"\"\"Man commits suicide after killing two women\"\"\",\n",
    "            \"\"\"Edhi Orange Line to be inaugurated today\"\"\",\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "\n",
    "sentimentdl = ClassifierDLModel.pretrained(name=MODEL_NAME)\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "pipelineModel = nlpPipeline.fit(empty_df)\n",
    "df = spark.createDataFrame(pd.DataFrame({\"text\":text_list}))\n",
    "result = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(F.explode(F.arrays_zip('document.result', 'sentiment.result')).alias(\"cols\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_age_news = \"bert_sequence_classifier_age_news\"\n",
    "model_hatexplain = \"bert_sequence_classifier_hatexplain\"\n",
    "model_emotion = \"bert_sequence_classifier_emotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_age_news = [\"\"\"600,000 flood victims have been shifted to camps: Murad\"\"\",\n",
    "            \"\"\"COAS hails US support for flood victims\"\"\",\n",
    "            \"\"\"My soul has just been pierced by the most evil look from @rickosborneorg. A mini panic attack &amp; chill in bones followed soon after.\"\"\",\n",
    "            \"\"\"Breach in Mirpur-Kotli highway on periphery of Mangla lake: officials\"\"\",\n",
    "            \"\"\"Two dead in SNGPL pipeline blast\"\"\",\n",
    "            \"\"\"Tire erupts at MQM’s former headquarters Nine Zero\"\"\",\n",
    "            \"\"\"A woman was gang raped by three persons including two policemen in Chuhng area\"\"\",\n",
    "            \"\"\"Man commits suicide after killing two women\"\"\",\n",
    "            \"\"\"Edhi Orange Line to be inaugurated today\"\"\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hatexplain = [\"\"\"600,000 flood victims have been shifted to camps: Murad\"\"\",\n",
    "            \"\"\"COAS hails US support for flood victims\"\"\",\n",
    "            \"\"\"My soul has just been pierced by the most evil look from @rickosborneorg. A mini panic attack &amp; chill in bones followed soon after.\"\"\",\n",
    "            \"\"\"Breach in Mirpur-Kotli highway on periphery of Mangla lake: officials\"\"\",\n",
    "            \"\"\"Two dead in SNGPL pipeline blast\"\"\",\n",
    "            \"\"\"Tire erupts at MQM’s former headquarters Nine Zero\"\"\",\n",
    "            \"\"\"A woman was gang raped by three persons including two policemen in Chuhng area\"\"\",\n",
    "            \"\"\"Man commits suicide after killing two women\"\"\",\n",
    "            \"\"\"Edhi Orange Line to be inaugurated today\"\"\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emotion = [\"\"\"600,000 flood victims have been shifted to camps: Murad\"\"\",\n",
    "            \"\"\"COAS hails US support for flood victims\"\"\",\n",
    "            \"\"\"My soul has just been pierced by the most evil look from @rickosborneorg. A mini panic attack &amp; chill in bones followed soon after.\"\"\",\n",
    "            \"\"\"Breach in Mirpur-Kotli highway on periphery of Mangla lake: officials\"\"\",\n",
    "            \"\"\"Two dead in SNGPL pipeline blast\"\"\",\n",
    "            \"\"\"Tire erupts at MQM’s former headquarters Nine Zero\"\"\",\n",
    "            \"\"\"A woman was gang raped by three persons including two policemen in Chuhng area\"\"\",\n",
    "            \"\"\"Man commits suicide after killing two women\"\"\",\n",
    "            \"\"\"Edhi Orange Line to be inaugurated today\"\"\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "              # model_age_news :text_age_news,\n",
    "              # model_hatexplain: text_hatexplain,\n",
    "              model_emotion: text_emotion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "def run_pipeline(model, text, results):  \n",
    "  document_assembler = DocumentAssembler() \\\n",
    "      .setInputCol('text') \\\n",
    "      .setOutputCol('document')\n",
    "\n",
    "  tokenizer = Tokenizer() \\\n",
    "      .setInputCols(['document']) \\\n",
    "      .setOutputCol('token')\n",
    "\n",
    "  sequenceClassifier = BertForSequenceClassification\\\n",
    "        .pretrained(model, 'en') \\\n",
    "        .setInputCols(['token', 'document']) \\\n",
    "        .setOutputCol('pred_class')\n",
    "\n",
    "  pipeline = Pipeline(stages=[document_assembler, tokenizer, sequenceClassifier])\n",
    "\n",
    "  df = spark.createDataFrame(text, StringType()).toDF(\"text\")\n",
    "  results[model]=(pipeline.fit(df).transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, text in zip(model_dict.keys(),model_dict.values()):\n",
    "  run_pipeline(model, text, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, result in zip(results.keys(),results.values()):  \n",
    "  res = result.select(F.explode(F.arrays_zip(result.document.result, \n",
    "                                                  result.pred_class.result,\n",
    "                                                  result.pred_class.metadata)).alias(\"col\"))\\\n",
    "                  .select(F.expr(\"col['1']\").alias(\"prediction\"),\n",
    "                          F.expr(\"col['2']\").alias(\"confidence\"),\n",
    "                          F.expr(\"col['0']\").alias(\"sentence\"))\n",
    "                  \n",
    "  if res.count()>1:\n",
    "    udf_func = F.udf(lambda x,y:  x[\"Some(\"+str(y)+\")\"])\n",
    "    print(\"\\n\",model_name,\"\\n\") \n",
    "    res.withColumn('confidence', udf_func(res.confidence, res.prediction)).show(truncate=False)\n",
    "    print(\"\\n**********************************\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = test_sentences = [\"\"\"Death toll reaches 1,136 across country as flood threat lingers in KP\"\"\",\\\n",
    "    \"\"\"Real faces' exposed: Miftah calls for Jhagra to resign, Tarin to quit politics after audio leaks\"\"\",\\\n",
    "        \"\"\"Gill open to issuing apology over controversial remarks, counsel tells Islamabad court in sedition case\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols(\"document\")\\\n",
    "  .setOutputCol(\"token\")  \n",
    "\n",
    "tokenClassifier = BertForTokenClassification.pretrained(\"bert_token_classifier_ner_btc\", \"en\")\\\n",
    "  .setInputCols(\"token\", \"document\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setCaseSensitive(True)\n",
    "\n",
    "ner_converter = NerConverter()\\\n",
    "        .setInputCols([\"document\",\"token\",\"ner\"])\\\n",
    "        .setOutputCol(\"ner_chunk\")\\\n",
    "        \n",
    "\n",
    "pipeline =  Pipeline(stages=[document, tokenizer, tokenClassifier, ner_converter])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(spark.createDataFrame(pd.DataFrame({'text': ['']})))\n",
    "\n",
    "result = model.transform(spark.createDataFrame(pd.DataFrame({'text': text_list})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result.select(F.explode(F.arrays_zip('document.result', 'ner_chunk.result',\"ner_chunk.metadata\")).alias(\"cols\")) \\\n",
    ".select(\n",
    "        F.expr(\"cols['1']\").alias(\"chunk\"),\n",
    "        F.expr(\"cols['2'].entity\").alias('result')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "for i in range(len(text_list)):\n",
    "  NerVisualizer().display(\n",
    "      result = result.collect()[i],\n",
    "      label_col = 'ner_chunk',\n",
    "      document_col = 'document'\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# to read parquet file\n",
    "df = sqlContext.read.parquet('../parc/')\n",
    "\n",
    "df.createOrReplaceTempView(\"ParquetTable\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols(\"document\")\\\n",
    "  .setOutputCol(\"token\")  \n",
    "\n",
    "tokenClassifier = BertForTokenClassification.pretrained(\"bert_token_classifier_ner_btc\", \"en\")\\\n",
    "  .setInputCols(\"token\", \"document\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setCaseSensitive(True)\n",
    "\n",
    "ner_converter = NerConverter()\\\n",
    "        .setInputCols([\"document\",\"token\",\"ner\"])\\\n",
    "        .setOutputCol(\"ner_chunk\")\\\n",
    "        \n",
    "\n",
    "pipeline =  Pipeline(stages=[document, tokenizer, tokenClassifier, ner_converter])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)\n",
    "\n",
    "result = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "NerVisualizer().display(\n",
    "    result = result.collect()[0],\n",
    "    label_col = 'ner_chunk',\n",
    "    document_col = 'document'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result.select(F.explode(F.arrays_zip('document.result', 'ner_chunk.result',\"ner_chunk\")).alias(\"cols\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.select(\"ner_chunk.result\",\"ner_chunk.metadata\").show(truncate=False)\n",
    "# result.select('text', F.explode('ner_chunk.metadata').alias('clean_text')).show(truncate=False)\n",
    "result.select('token.result','ner.result').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sparknlp_display import NerVisualizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"nerdl_fewnerd_100d\",\"nerdl_fewnerd_subentity_100d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for MODEL_NAME in model_list:\n",
    "  documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "  sentenceDetector = SentenceDetector()\\\n",
    "        .setInputCols([\"document\"])\\\n",
    "        .setOutputCol(\"sentence\")\n",
    "\n",
    "  tokenizer = Tokenizer()\\\n",
    "        .setInputCols([\"sentence\"])\\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "  embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\")\\\n",
    "            .setInputCols(\"sentence\", \"token\") \\\n",
    "            .setOutputCol(\"embeddings\")\n",
    "\n",
    "  ner = NerDLModel.pretrained(MODEL_NAME)\\\n",
    "          .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "          .setOutputCol(\"ner\")\n",
    "\n",
    "  ner_converter = NerConverter()\\\n",
    "      .setInputCols(['document', 'token', 'ner'])\\\n",
    "      .setOutputCol('ner_chunk')\n",
    "\n",
    "  nlpPipeline = Pipeline(stages=[documentAssembler, sentenceDetector,\n",
    "        tokenizer,\n",
    "        embeddings,\n",
    "        ner,\n",
    "        ner_converter])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"\"\"Seven injured in Quetta grenade attack\"\"\",\n",
    "    \"\"\"Disaster orthodoxy in Pakistan — floods aren't God's fury, we're the culprits and inaction is our sin\"\"\"\n",
    "]\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "ner_model = nlpPipeline.fit(empty_data)\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame({'text': text_list}))\n",
    "\n",
    "result = ner_model.transform(df)\n",
    "print(\"<----------------- MODEL NAME:\",\"\\033[1m\" + MODEL_NAME + \"\\033[0m\",\" ----------------- >\")\n",
    "NerVisualizer().display(\n",
    "      result = result.collect()[1],\n",
    "      label_col = 'ner_chunk',\n",
    "      document_col = 'document'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoCoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'place_id': 1274860, 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright', 'osm_type': 'node', 'osm_id': 316440978, 'boundingbox': ['33.5338118', '33.8538118', '72.9051511', '73.2251511'], 'lat': '33.6938118', 'lon': '73.0651511', 'display_name': 'اسلام آباد, وفاقی دارالحکومت اسلام آباد, 44000, پاکستان', 'class': 'place', 'type': 'city', 'importance': 0.5841564917637788, 'icon': 'https://nominatim.openstreetmap.org/ui/mapicons/poi_place_city.p.20.png'}\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"ehsan\")\n",
    "location = geolocator.geocode(\"Islamabad\")\n",
    "# print(location == None)\n",
    "# print(location.address)\n",
    "\n",
    "# print([location.latitude, location.longitude])\n",
    "\n",
    "print(location.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;head&gt;    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_23c7b0e773e80af5bc080934297dc4a5 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_23c7b0e773e80af5bc080934297dc4a5&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;    \n",
       "    \n",
       "            var map_23c7b0e773e80af5bc080934297dc4a5 = L.map(\n",
       "                &quot;map_23c7b0e773e80af5bc080934297dc4a5&quot;,\n",
       "                {\n",
       "                    center: [30.8091281, 73.4493301],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 14,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_cacc69797e74874eb9c5ce0dbb719b2a = L.tileLayer(\n",
       "                &quot;https://cartodb-basemaps-{s}.global.ssl.fastly.net/light_all/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors \\u0026copy; \\u003ca href=\\&quot;http://cartodb.com/attributions\\&quot;\\u003eCartoDB\\u003c/a\\u003e, CartoDB \\u003ca href =\\&quot;http://cartodb.com/attributions\\&quot;\\u003eattributions\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_23c7b0e773e80af5bc080934297dc4a5);\n",
       "        \n",
       "&lt;/script&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f5868d1c520>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "map1 = folium.Map(\n",
    "    location=[30.8091281, 73.4493301],\n",
    "    tiles='cartodbpositron',\n",
    "    zoom_start=14,\n",
    ")\n",
    "# df.apply(lambda row:folium.CircleMarker(location=[row[\"latitude\"], row[\"longitude\"]]).add_to(map1), axis=1)\n",
    "map1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import csv\n",
    "maxTweets = 100\n",
    "\n",
    "keyword = 'MinusOneNaManzoor'\n",
    "#place = '5e02a0f0d91c76d2' #This geo_place string corresponds to İstanbul, Turkey on twitter.\n",
    "\n",
    "#keyword = 'covid'\n",
    "#place = '01fbe706f872cb32' This geo_place string corresponds to Washington DC on twitter.\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('place_result.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id','date','tweet',]) \n",
    "\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('MinusOneNaManzoor + since:2022-08-25 until:2022-09-10 -filter:links -filter:replies').get_items()):\n",
    "        if i > maxTweets :\n",
    "            break  \n",
    "        csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/11 13:48:44 WARN Utils: Your hostname, zainab-ThinkPad-T560 resolves to a loopback address: 127.0.1.1; using 192.168.178.29 instead (on interface wlp4s0)\n",
      "22/09/11 13:48:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/zainab/miniconda3/envs/sparknlp/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/zainab/.ivy2/cache\n",
      "The jars for the packages stored in: /home/zainab/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d1fac87f-04e9-4030-bf0c-b95e7f2b1bdf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in local-m2-cache\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 1340ms :: artifacts dl 76ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from local-m2-cache in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d1fac87f-04e9-4030-bf0c-b95e7f2b1bdf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/32ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/11 13:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/11 13:49:02 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/11 13:49:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "22/09/11 13:49:12 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/11 13:49:12 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"جیل جانے کیلئے تیار ہوں، بیگ تیار کر رکھا ہے لیکن میری گرفتاری کے بعد جو کچھ بھی ہوگا اُس کا ذمہ دار میں نہیں ہوں گا، عمران خان\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 514.9 KB\n",
      "[ | ]sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 514.9 KB\n",
      "Download done! Loading the resource.\n",
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 13:50:07.491093: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/zainab/miniconda3/envs/sparknlp/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.3.0.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "translate_ur_en download started this may take some time.\n",
      "Approximate size to download 275.2 MB\n",
      "[ | ]translate_ur_en download started this may take some time.\n",
      "Approximate size to download 275.2 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "\n",
      "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n",
      ": java.lang.NoSuchMethodException: org.apache.spark.ml.PipelineModel.<init>(java.lang.String)\n",
      "\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n",
      "\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:468)\n",
      "\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:31)\n",
      "\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:500)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:492)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadModel(ResourceDownloader.scala:666)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel(ResourceDownloader.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[OK!]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n: java.lang.NoSuchMethodException: org.apache.spark.ml.PipelineModel.<init>(java.lang.String)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:468)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:31)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:500)\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:492)\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadModel(ResourceDownloader.scala:666)\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel(ResourceDownloader.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m## More accurate Sentence Detection using Deep Learning\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sentencerDL \u001b[39m=\u001b[39m SentenceDetectorDLModel()\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m.\u001b[39mpretrained(\u001b[39m\"\u001b[39m\u001b[39msentence_detector_dl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mxx\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m.\u001b[39msetInputCols([\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m])\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m marian \u001b[39m=\u001b[39m MarianTransformer\u001b[39m.\u001b[39;49mpretrained(\u001b[39m\"\u001b[39;49m\u001b[39mtranslate_ur_en\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mxx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m.\u001b[39msetInputCols([\u001b[39m\"\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m\"\u001b[39m])\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m nlp_pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     documentAssembler, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     sentencerDL, marian\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#X66sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ])\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/sparknlp/annotator/seq2seq/marian_transformer.py:249\u001b[0m, in \u001b[0;36mMarianTransformer.pretrained\u001b[0;34m(name, lang, remote_loc)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m\"\"\"Downloads and loads a pretrained model.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39m    The restored model\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrained\u001b[39;00m \u001b[39mimport\u001b[39;00m ResourceDownloader\n\u001b[0;32m--> 249\u001b[0m \u001b[39mreturn\u001b[39;00m ResourceDownloader\u001b[39m.\u001b[39;49mdownloadModel(MarianTransformer, name, lang, remote_loc)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/sparknlp/pretrained/resource_downloader.py:43\u001b[0m, in \u001b[0;36mResourceDownloader.downloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     42\u001b[0m     sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     44\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     stop_threads \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/sparknlp/pretrained/resource_downloader.py:40\u001b[0m, in \u001b[0;36mResourceDownloader.downloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     38\u001b[0m t1\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     j_obj \u001b[39m=\u001b[39m _internal\u001b[39m.\u001b[39;49m_DownloadModel(reader\u001b[39m.\u001b[39;49mname, name, language, remote_loc, j_dwn)\u001b[39m.\u001b[39mapply()\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     42\u001b[0m     sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/sparknlp/internal/__init__.py:317\u001b[0m, in \u001b[0;36m_DownloadModel.__init__\u001b[0;34m(self, reader, name, language, remote_loc, validator)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, reader, name, language, remote_loc, validator):\n\u001b[0;32m--> 317\u001b[0m     \u001b[39msuper\u001b[39;49m(_DownloadModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcom.johnsnowlabs.nlp.pretrained.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m validator \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.downloadModel\u001b[39;49m\u001b[39m\"\u001b[39;49m, reader,\n\u001b[1;32m    318\u001b[0m                                          name, language, remote_loc)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/sparknlp/internal/extended_java_wrapper.py:26\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39msuper\u001b[39m(ExtendedJavaWrapper, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(java_obj)\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_java_obj(java_obj, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/sparknlp/internal/extended_java_wrapper.py:36\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_java_obj\u001b[39m(\u001b[39mself\u001b[39m, java_class, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(java_class, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m java_obj(\u001b[39m*\u001b[39;49mjava_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparknlp/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n: java.lang.NoSuchMethodException: org.apache.spark.ml.PipelineModel.<init>(java.lang.String)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:468)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:31)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:500)\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:492)\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadModel(ResourceDownloader.scala:666)\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel(ResourceDownloader.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    ".setInputCol(\"text\")\\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "## More accurate Sentence Detection using Deep Learning\n",
    "sentencerDL = SentenceDetectorDLModel()\\\n",
    ".pretrained(\"sentence_detector_dl\", \"xx\")\\\n",
    ".setInputCols([\"document\"])\\\n",
    ".setOutputCol(\"sentences\")\n",
    "\n",
    "marian = MarianTransformer.pretrained(\"translate_ur_en\", \"xx\")\\\n",
    ".setInputCols([\"sentences\"])\\\n",
    ".setOutputCol(\"translation\")\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    documentAssembler, \n",
    "    sentencerDL, marian\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m empty_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame([[\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m]])\u001b[39m.\u001b[39mtoDF(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pipeline_model \u001b[39m=\u001b[39m nlp_pipeline\u001b[39m.\u001b[39mfit(empty_df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lmodel \u001b[39m=\u001b[39m LightPipeline(pipeline_model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zainab/Downloads/tweepy/airflow-spark/notebooks/hello-world-notebook.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m res \u001b[39m=\u001b[39m lmodel\u001b[39m.\u001b[39mfullAnnotate(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "pipeline_model = nlp_pipeline.fit(empty_df)\n",
    "lmodel = LightPipeline(pipeline_model)\n",
    "res = lmodel.fullAnnotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Original:', text, '\\n\\n')\n",
    "\n",
    "print ('Translated:\\n')\n",
    "for sentence in res[0]['translation']:\n",
    "  print (sentence.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate_ur_en download started this may take some time.\n",
      "Approx size to download 275.2 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'document': ['جیل جانے کیلئے تیار ہوں، بیگ تیار کر رکھا ہے لیکن میری گرفتاری کے بعد جو کچھ بھی ہوگا اُس کا ذمہ دار میں نہیں ہوں گا، عمران خان'],\n",
       " 'sentence': ['جیل جانے کیلئے تیار ہوں، بیگ تیار کر رکھا ہے لیکن میری گرفتاری کے بعد جو کچھ بھی ہوگا اُس کا ذمہ دار میں نہیں ہوں گا، عمران خان'],\n",
       " 'translation': [\"I'm ready to go to prison, and I'm ready to keep the bag, but I'm not responsible for what I'm going to do after my arrest, despite my\"]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline \n",
    "pipeline = PretrainedPipeline(\"translate_ur_en\", lang = \"xx\") \n",
    "pipeline.fullAnnotate(\"جیل جانے کیلئے تیار ہوں، بیگ تیار کر رکھا ہے لیکن میری گرفتاری کے بعد جو کچھ بھی ہوگا اُس کا ذمہ دار میں نہیں ہوں گا، عمران خان\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['inus 1 formula is strictly rejected by people of Pakistan'],\n",
       " 'sentence': ['inus 1 formula is strictly rejected by people of Pakistan'],\n",
       " 'translation': ['Flystine Laun Floudi Launun Floudi Lapri bute']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 19:26:08 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 704306213 ms exceeds timeout 120000 ms\n",
      "22/09/19 19:26:08 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "pipeline.annotate(\"inus 1 formula is strictly rejected by people of Pakistan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"تم سے سنبھالا نہیں جائے گا یہ شوق گلے پڑ جائے گا\"\"\"\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "lmodel = LightPipeline(pipeline_model)\n",
    "res = lmodel.fullAnnotate(text)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sparknlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf269ee04d05c42aad91061578575413d6682be793a5e82bde75bfd08faaead9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
